{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d51744",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers datasets faiss-cpu pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467beacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ==========================================\n",
    "# 1. PRÉPARATION DES DONNÉES (Dataset STSB)\n",
    "# ==========================================\n",
    "print(\"Chargement du dataset STSB...\")\n",
    "# On utilise la version anglaise par défaut, remplacez \"en\" par \"fr\" si besoin\n",
    "dataset = load_dataset(\"stsb_multi_mt\", \"en\", split=\"test\")\n",
    "\n",
    "# Pour simuler un moteur de recherche :\n",
    "# queries = les phrases d'entrée\n",
    "# corpus = la base de données dans laquelle on cherche\n",
    "queries = dataset[\"sentence1\"]\n",
    "corpus = dataset[\"sentence2\"]\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONFIGURATION DES MODÈLES À TESTER\n",
    "# ==========================================\n",
    "# On teste 3 tailles de modèles pour comparer Précision vs Vitesse\n",
    "models_to_test = [\n",
    "    \"all-MiniLM-L6-v2\",             # Très rapide, léger\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\", # Supporte le français\n",
    "    \"all-mpnet-base-v2\"             # Très précis, plus lent\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# ==========================================\n",
    "# 3. BOUCLE DE BENCHMARK\n",
    "# ==========================================\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n--- Évaluation du modèle : {model_name} ---\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Mesure du temps d'encodage (Latence)\n",
    "    start_time = time.time()\n",
    "    corpus_embeddings = model.encode(corpus, convert_to_numpy=True, show_progress_bar=True)\n",
    "    query_embeddings = model.encode(queries, convert_to_numpy=True, show_progress_bar=True)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    avg_latency = (total_time / (len(queries) + len(corpus))) * 1000 # en millisecondes\n",
    "\n",
    "    # Normalisation pour la similarité cosinus (obligatoire pour FAISS IndexFlatIP)\n",
    "    faiss.normalize_L2(corpus_embeddings)\n",
    "    faiss.normalize_L2(query_embeddings)\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. INDEXATION FAISS\n",
    "    # ==========================================\n",
    "    dimension = corpus_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension) # Recherche exacte par produit scalaire\n",
    "    index.add(corpus_embeddings)\n",
    "\n",
    "    # Recherche des 5 meilleurs résultats (Top-k)\n",
    "    k = 5\n",
    "    distances, indices = index.search(query_embeddings, k)\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. CALCUL DES MÉTRIQUES (Recall & MRR)\n",
    "    # ==========================================\n",
    "    recall_at_1 = 0\n",
    "    recall_at_5 = 0\n",
    "    mrr = 0\n",
    "\n",
    "    for i in range(len(queries)):\n",
    "        # La vérité terrain : pour la requête i, on veut retrouver l'index i du corpus\n",
    "        target_idx = i\n",
    "        retrieved_indices = indices[i]\n",
    "\n",
    "        # Recall@1\n",
    "        if target_idx == retrieved_indices[0]:\n",
    "            recall_at_1 += 1\n",
    "\n",
    "        # Recall@5\n",
    "        if target_idx in retrieved_indices:\n",
    "            recall_at_5 += 1\n",
    "            # MRR : 1 / position du bon résultat (1-indexed)\n",
    "            rank = np.where(retrieved_indices == target_idx)[0][0] + 1\n",
    "            mrr += 1 / rank\n",
    "\n",
    "    # Moyennes finales\n",
    "    num_queries = len(queries)\n",
    "    all_results.append({\n",
    "        \"Modèle\": model_name,\n",
    "        \"Recall@1\": round(recall_at_1 / num_queries, 4),\n",
    "        \"Recall@5\": round(recall_at_5 / num_queries, 4),\n",
    "        \"MRR\": round(mrr / num_queries, 4),\n",
    "        \"Latence (ms/emb)\": round(avg_latency, 2),\n",
    "        \"Dimensions\": dimension\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# 6. AFFICHAGE DES RÉSULTATS MLOPS\n",
    "# ==========================================\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TABLEAU COMPARATIF FINAL\")\n",
    "print(\"=\"*50)\n",
    "print(df_results.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
